# Chatbot de Recherche SÃ©mantique arXiv

## AperÃ§u

Le **Chatbot de Recherche SÃ©mantique arXiv** est une application web intelligente qui permet d'explorer et de rechercher dans une vaste collection d'articles scientifiques provenant d'arXiv. Utilisant des techniques avancÃ©es de traitement du langage naturel et de recherche sÃ©mantique, ce projet offre une interface conversationnelle intuitive pour dÃ©couvrir des recherches pertinentes.

Ce projet rÃ©sout le problÃ¨me de la surcharge d'information dans la littÃ©rature scientifique en permettant aux chercheurs, Ã©tudiants et professionnels de poser des questions en langage naturel et d'obtenir des rÃ©ponses synthÃ©tisÃ©es basÃ©es sur des milliers d'articles scientifiques. L'application combine la puissance des embeddings sÃ©mantiques, de l'indexation vectorielle FAISS et d'une interface utilisateur moderne construite avec Streamlit.

### FonctionnalitÃ©s ClÃ©s
- **Recherche SÃ©mantique AvancÃ©e** : Recherche par similaritÃ© de sens plutÃ´t que par mots-clÃ©s
- **Interface Conversationnelle** : Chat interactif avec rÃ©ponses dÃ©taillÃ©es et contextualisÃ©es
- **SynthÃ¨se Intelligente** : GÃ©nÃ©ration automatique de rÃ©ponses complÃ¨tes basÃ©es sur multiple articles
- **Visualisations Interactives** : Statistiques et analyses graphiques du dataset
- **DÃ©ploiement Cloud** : Application accessible via Streamlit Cloud

## Architecture du SystÃ¨me

Le systÃ¨me suit une architecture modulaire en plusieurs Ã©tapes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   arXiv API     â”‚â”€â”€â”€â–¶â”‚  Extraction &    â”‚â”€â”€â”€â–¶â”‚   Nettoyage     â”‚
â”‚                 â”‚    â”‚  Collecte        â”‚    â”‚   des DonnÃ©es   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Interface Web   â”‚â—€â”€â”€â”€â”‚  Recherche       â”‚â—€â”€â”€â”€â”‚  GÃ©nÃ©ration     â”‚
â”‚ (Streamlit)     â”‚    â”‚  SÃ©mantique      â”‚    â”‚  d'Embeddings   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Index FAISS    â”‚â—€â”€â”€â”€â”‚  Sentence       â”‚
                       â”‚                  â”‚    â”‚  Transformers   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Structure du Projet

```
Scoupus-chatbot/
â”œâ”€â”€ ğŸ“‚ app/                       # Applications Streamlit
â”‚   â”œâ”€â”€ beta_hatbot.py            # Application principale
â”‚   â””â”€â”€ pro_chatbot.py            # Version professionnelle
â”œâ”€â”€ ğŸ“‚ data/                      # DonnÃ©es et scripts
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/               # Scripts de traitement
â”‚   â”‚   â”œâ”€â”€ arxiv_extractor.py    # Extraction des donnÃ©es arXiv
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py       # Nettoyage des donnÃ©es
â”‚   â”‚   â””â”€â”€ semantic_indexer.py   # CrÃ©ation de l'index sÃ©mantique
â”‚   â”œâ”€â”€ ğŸ“‚ data_source/           # DonnÃ©es brutes extraites
â”‚   â”œâ”€â”€ ğŸ“‚ processed/             # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ ğŸ“‚ search_index/          # Index de recherche FAISS
â”œâ”€â”€ ğŸ“‚ config/                    # Configuration
â”‚   â””â”€â”€ config.py                 # ParamÃ¨tres de configuration
â”œâ”€â”€ ğŸ“„ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ .env                       # Variables d'environnement
â”œâ”€â”€ ğŸ“„ .gitignore                 # Fichiers Ã  ignorer par Git
â””â”€â”€ ğŸ“„ README.md                  # Documentation du projet
```

## Installation et Configuration

### PrÃ©requis
- **Python 3.8+** (recommandÃ© : Python 3.9 ou 3.10)
- **Git** pour cloner le repository
- **Environnement virtuel** (venv ou conda)
- **8GB+ RAM** recommandÃ©s pour le traitement des embeddings

### Installation

1. **Cloner le repository**
```bash
  git clone https://github.com/ElazzouziHassan/scopus_chatbot.git
  cd scopus_chatbot
```

2. **CrÃ©er un environnement virtuel**
```bash
  # Avec venv
  python -m venv venv
  source venv/bin/activate  # Sur Windows: venv\Scripts\activate

  # Ou avec conda
  conda create -n arxiv-chatbot python=3.9
  conda activate arxiv-chatbot
```

3. **Installer les dÃ©pendances**
```bash
  pip install -r requirements.txt
```

4. **Configurer l'environnement**
```bash
  cp .env.example .env
  # Ã‰diter le fichier .env avec vos paramÃ¨tres
```

## Construction de l'Index SÃ©mantique

### Ã‰tape 1 : Extraction des DonnÃ©es

```bash
  # Extraction automatique (recommandÃ©e)
  python data/scripts/arxiv_extractor.py

  # Ou extraction personnalisÃ©e
  python data/scripts/arxiv_extractor.py \
    --query "machine learning" \
    --max_results 1000 \
    --output data/data_source/ml_papers.json
```

### Ã‰tape 2 : Nettoyage des DonnÃ©es

```bash
  # Nettoyage automatique
  python data/scripts/data_cleaner.py --auto

  # Ou nettoyage personnalisÃ©
  python data/scripts/data_cleaner.py \
    --input data/data_source/raw_data.json \
    --output data/processed/clean_data.json \
    --stats
```

### Ã‰tape 3 : GÃ©nÃ©ration de l'Index SÃ©mantique

```bash
  # CrÃ©ation automatique de l'index
  python data/scripts/semantic_indexer.py --auto

  # Ou crÃ©ation personnalisÃ©e
  python data/scripts/semantic_indexer.py \
    --input data/processed/clean_data.json \
    --output data/search_index \
    --model all-MiniLM-L6-v2 \
    --test
```

**ModÃ¨les d'embeddings supportÃ©s :**
- `all-MiniLM-L6-v2` : Rapide, bonne qualitÃ© (384 dimensions)
- `all-mpnet-base-v2` : Meilleure qualitÃ©, plus lent (768 dimensions)
- `all-roberta-large-v1` : QualitÃ© optimale, trÃ¨s lent (1024 dimensions)

## Lancement du Chatbot

### ExÃ©cution Locale

```bash
  # Lancer l'application principale
  streamlit run app/beta_chatbot.py

  # Ou la version professionnelle
  streamlit run app/pro_chatbot.py
```

### Comportement Attendu

1. **Interface de Chat** : Interface conversationnelle avec historique des messages
2. **Recherche SÃ©mantique** : Recherche intelligente basÃ©e sur le sens des requÃªtes
3. **RÃ©ponses SynthÃ©tisÃ©es** : RÃ©ponses dÃ©taillÃ©es combinant plusieurs sources
4. **RÃ©sultats DÃ©taillÃ©s** : AccÃ¨s aux articles originaux avec scores de pertinence
5. **Statistiques** : Visualisations interactives du dataset

### Exemples de RequÃªtes

```
  "Qu'est-ce que l'apprentissage par renforcement ?"
  "DerniÃ¨res avancÃ©es en vision par ordinateur"
  "Applications pratiques des transformers"
  "MÃ©thodes d'optimisation en deep learning"
  "Recherches de Geoffrey Hinton sur les rÃ©seaux de neurones"
```

## DÃ©ploiement sur Streamlit Cloud

### Ã‰tapes de DÃ©ploiement

1. **PrÃ©parer le Repository**
```bash
  # S'assurer que les fichiers essentiels sont prÃ©sents
  git add app/pro_chatbot.py requirements.txt README.md
  git commit -m "PrÃªt pour le dÃ©ploiement"
  git push origin main
```

2. **Configurer Streamlit Cloud**
- Aller sur [share.streamlit.io](https://share.streamlit.io)
- Connecter votre repository GitHub
- SÃ©lectionner `app/chatbot.py` comme fichier principal
- DÃ©ployer l'application

3. **Variables d'Environnement**
```toml
  # Dans .streamlit/secrets.toml
  [general]
  ARXIV_API_RATE_LIMIT = 3
  DEFAULT_MODEL = "all-MiniLM-L6-v2"
```

### Application DÃ©ployÃ©e
[Lien vers l'application dÃ©ployÃ©e](https://votre-app.streamlit.app)

## Tests et Validation

### Tests de Performance

```bash
  # Test de l'extraction
  python data/scripts/arxiv_extractor.py --query "test" --max_results 10

  # Test de l'index sÃ©mantique
  python data/scripts/semantic_indexer.py --test
```

### MÃ©triques de Validation

- **Temps de RÃ©ponse** : < 2 secondes pour les requÃªtes standard
- **PrÃ©cision SÃ©mantique** : Ã‰valuÃ©e sur un ensemble de test de 100 requÃªtes
- **Couverture** : Plus de 30,000 articles scientifiques indexÃ©s
- **Satisfaction Utilisateur** : Interface intuitive et rÃ©ponses pertinentes

## MÃ©triques de Performance

| MÃ©trique | Valeur | Description |
|----------|--------|-------------|
| **Articles IndexÃ©s** | 30,000+ | Nombre total d'articles dans la base |
| **Temps de Recherche** | < 100ms | Temps moyen de recherche sÃ©mantique |
| **PrÃ©cision@10** | 85%+ | Pertinence des 10 premiers rÃ©sultats |
| **Couverture Temporelle** | 2010-2024 | PÃ©riode couverte par les articles |
| **Domaines** | 20+ | Nombre de domaines scientifiques |

## Mise Ã  Jour de l'Index

### Actualisation ComplÃ¨te

```bash
  # Pipeline complet de mise Ã  jour
  python data/scripts/arxiv_extractor.py --large_scale --target_size 2.0
  python data/scripts/data_cleaner.py --auto
  python data/scripts/semantic_indexer.py --auto
```

### Actualisation IncrÃ©mentale

```bash
  # Ajouter de nouveaux articles
  python data/scripts/arxiv_extractor.py \
    --query "submittedDate:[20240101 TO 20241231]" \
    --max_results 5000 \
    --output data/data_source/new_papers.json
```

## SÃ©curitÃ© et ConfidentialitÃ©

### Gestion des Secrets

- **Variables d'environnement** : Utilisation de fichiers `.env` pour les configurations sensibles
- **Streamlit Secrets** : Configuration sÃ©curisÃ©e pour le dÃ©ploiement cloud
- **DonnÃ©es Publiques** : Utilisation exclusive de donnÃ©es publiques d'arXiv

### Bonnes Pratiques

- Ne jamais commiter de clÃ©s API dans le repository
- Utiliser des environnements virtuels isolÃ©s
- Respecter les limites de taux de l'API arXiv (3 requÃªtes/seconde)

## DÃ©pannage

### Erreurs Communes

**1. Erreur d'importation de SemanticSearcher**
```bash
  # Solution : VÃ©rifier que l'index est construit
  python data/scripts/semantic_indexer.py --auto
```

**2. Fichiers trop volumineux pour GitHub**
```bash
  # Solution : Utiliser Git LFS ou exclure les gros fichiers
  git rm --cached data/search_index/papers_data.json
  echo "data/search_index/papers_data.json" >> .gitignore
```

**3. MÃ©moire insuffisante**
```bash
  # Solution : RÃ©duire la taille du batch ou utiliser un modÃ¨le plus petit
  # Modifier batch_size=16 dans semantic_indexer.py
```

**4. Erreur de connexion Ã  l'API arXiv**
```bash
  # Solution : VÃ©rifier la connexion internet et respecter les limites de taux
  # Attendre 3 secondes entre les requÃªtes
```

## Travaux Futurs / Feuille de Route

### AmÃ©liorations PrÃ©vues

- [ ] **IntÃ©gration Multi-Sources** : Ajout de PubMed, IEEE Xplore
- [ ] **Recherche Multimodale** : Support des images et graphiques
- [ ] **Personnalisation** : Profils utilisateur et recommandations
- [ ] **API REST** : Interface programmatique pour dÃ©veloppeurs
- [ ] **Analyse de Citations** : Graphe de citations et impact des articles
- [ ] **Support Multilingue** : Interface en plusieurs langues
- [ ] **Collaboration** : Partage de collections et annotations

### Cas d'Usage Potentiels

- **Recherche AcadÃ©mique** : Aide Ã  la revue de littÃ©rature
- **Veille Technologique** : Suivi des derniÃ¨res innovations
- **Ã‰ducation** : Support pÃ©dagogique pour Ã©tudiants
- **R&D Industrielle** : Exploration de nouvelles technologies

## Contributeurs

### Ã‰quipe Principale

- **Groupe 2IAD (H. Elazzouzi, K. Ettalbi & O. Rochdi)** 
  - DÃ©veloppeurs Principal
  - Architecture du systÃ¨me
  - ImplÃ©mentation des algorithmes de recherche
  - Interface utilisateur Streamlit

### Contributions Bienvenues

Nous accueillons les contributions de la communautÃ© ! Voici comment contribuer :

1. **Fork** le repository
2. **CrÃ©er** une branche pour votre fonctionnalitÃ© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. **Commiter** vos changements (`git commit -am 'Ajout nouvelle fonctionnalitÃ©'`)
4. **Pousser** vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. **CrÃ©er** une Pull Request

### Types de Contributions RecherchÃ©es

- ğŸ› Correction de bugs
- âœ¨ Nouvelles fonctionnalitÃ©s
- ğŸ“š AmÃ©lioration de la documentation
- ğŸ§ª Tests et validation
- ğŸŒ Traductions
- ğŸ¨ AmÃ©liorations de l'interface

## Licence

Ce projet est sous licence **MIT License**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

```
MIT License

Copyright (c) 2025 2IAD

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

## RÃ©fÃ©rences et Remerciements

### Technologies UtilisÃ©es

- **[arXiv API](https://arxiv.org/help/api)** - Source des donnÃ©es scientifiques
- **[Sentence Transformers](https://www.sbert.net/)** - GÃ©nÃ©ration d'embeddings sÃ©mantiques
- **[FAISS](https://faiss.ai/)** - Indexation et recherche vectorielle rapide
- **[Streamlit](https://streamlit.io/)** - Framework d'interface web
- **[Plotly](https://plotly.com/)** - Visualisations interactives
- **[Pandas](https://pandas.pydata.org/)** - Manipulation de donnÃ©es

### Bibliographie Scientifique

1. Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *EMNLP 2019*.
2. Johnson, J., Douze, M., & JÃ©gou, H. (2019). Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*.
3. Karpukhin, V., et al. (2020). Dense Passage Retrieval for Open-Domain Question Answering. *EMNLP 2020*.

### Remerciements SpÃ©ciaux

- **arXiv.org** pour l'accÃ¨s libre aux publications scientifiques
- **Hugging Face** pour les modÃ¨les de transformers prÃ©-entraÃ®nÃ©s
- **La communautÃ© open-source** pour les outils et bibliothÃ¨ques utilisÃ©s

---

## DÃ©marrage Rapide

```bash
  # Installation rapide
  git clone https://github.com/ElazzouziHassan/scopus_chatbot.git
  cd scopus_chatbot
  python -m venv venv && source venv/bin/activate
  pip install -r requirements.txt

  # Construction de l'index (peut prendre 30-60 minutes)
  python data/scripts/arxiv_extractor.py
  python data/scripts/data_cleaner.py --auto
  python data/scripts/semantic_indexer.py --auto

  # Lancement de l'application
  streamlit run app/pro_chatbot.py
```

** Votre chatbot de recherche sÃ©mantique arXiv est maintenant prÃªt ! ğŸ‰**

---

*Pour toute question ou support, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub ou Ã  nous contacter directement.*
